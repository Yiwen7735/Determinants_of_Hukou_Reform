{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tika.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_dir(pdf_dir):\n",
    "    txt_dir = f\"{pdf_dir}_txt\"\n",
    "    if not os.path.exists(txt_dir):\n",
    "        os.mkdir(txt_dir)\n",
    "\n",
    "    for idx, pdf_file in enumerate(os.listdir(pdf_dir)):\n",
    "        if not pdf_file.endswith('.pdf'):\n",
    "            continue\n",
    "\n",
    "        # Copy file to ASCII file path\n",
    "        new_pdf_file = os.path.join(txt_dir, f\"{idx}.pdf\")\n",
    "        shutil.copyfile(os.path.join(pdf_dir, pdf_file), new_pdf_file)\n",
    "\n",
    "        # Extract text from PDF\n",
    "        data = tika.parser.from_file(new_pdf_file)\n",
    "\n",
    "        # Write extracted text back to file\n",
    "        txt_file = os.path.join(txt_dir, pdf_file[:-3] + \"txt\")\n",
    "        with open(txt_file, 'w', encoding='utf8') as fh:\n",
    "            fh.write(data['content'])\n",
    "\n",
    "        # Remove temporary ASCII file PDF\n",
    "        os.remove(new_pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"Articles_2010\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))\n",
    "    \n",
    "root_dir = \"Articles_2011\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))  \n",
    "\n",
    "root_dir = \"Articles_2012\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))\n",
    "\n",
    "root_dir = \"Articles_2013\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))\n",
    "    \n",
    "root_dir = \"Articles_2014\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))\n",
    "    \n",
    "root_dir = \"Articles_2015\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))\n",
    "    \n",
    "root_dir = \"Articles_2016\"\n",
    "for pdf_dir in os.listdir(root_dir):\n",
    "    process_pdf_dir(os.path.join(root_dir, pdf_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import jieba\n",
    "from functools import reduce\n",
    "\n",
    "#DataFrame(dtype=float)\n",
    "# Get all the files in a dataframe\n",
    "\n",
    "def openfile(proname, year):\n",
    "    path = r\"C://Users//User//Desktop//SentimentAnalysis//Articles_\" + year + \"//\" + proname + \"_\" + year + \"_txt\"\n",
    "    files= glob.glob(path + \"/*.txt\")\n",
    "\n",
    "    t = []\n",
    "    for file in files:\n",
    "        text = open(file,'r',encoding = 'utf8').read().strip().replace('\\n', '').replace(\" \", \"\")\n",
    "        t.append([proname,text])\n",
    "        \n",
    "    return t\n",
    "\n",
    "province = ['Anhui','Beijing','Chongqing','Fujian','Gansu','Guangdong','Guangxi','Guizhou','Hainan','Hebei','Heilongjiang',\n",
    "            'Henan','Hubei','Hunan','Inner Mongolia','Jiangsu','Jiangxi','Jilin','Liaoning','Qinghai','Shaanxi','Shandong',\n",
    "            'Shanghai','Shanxi','Sichuan','Tianjin','Xinjiang','Xizang','Yunnan','Zhejiang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendf(year):\n",
    "    \n",
    "    dfs = [pd.DataFrame(openfile(i, year)) for i in province]\n",
    "    df = reduce(lambda left, right: left.append(right), dfs).rename(columns = {0:'Province',1:'Articles'})\n",
    "    \n",
    "    # Number the articles of each year for future sampling purpose\n",
    "    df['#'] = range(1,len(df) + 1)\n",
    "    df = df.reset_index().set_index('#').drop('index', axis = 1)\n",
    "    \n",
    "    # Tokenize the articles using jieba\n",
    "    df['Text'] = df['Articles'].apply(lambda row: \" \".join(jieba.cut(row, cut_all = False)))\n",
    "    \n",
    "    # Generate an extra tokenized column with all the stop words removed\n",
    "    stop_words = open('Chinese_Stop_Words', 'r', encoding = 'utf8').read().split('\\n')\n",
    "    df['Text_truncated'] = df['Text'].apply(lambda row: \" \".join([token for token in row.split(\" \") if token not in stop_words]))\n",
    "\n",
    "    df['Year'] = year\n",
    "    df['pos'] = None\n",
    "    df['dis'] = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.134 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "df_16 = gendf('2016')\n",
    "df_15 = gendf('2015')\n",
    "df_14 = gendf('2014')\n",
    "df_13 = gendf('2013')\n",
    "df_12 = gendf('2012')\n",
    "df_11 = gendf('2011')\n",
    "df_10 = gendf('2010')\n",
    "\n",
    "# Combine all six years' data together and save it as a csv file for future use\n",
    "\n",
    "dfs = [df_10, df_11, df_12, df_13, df_14, df_15, df_16]\n",
    "df = reduce(lambda top, bottom: top.append(bottom), dfs)\n",
    "\n",
    "#df.to_csv('savedf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
